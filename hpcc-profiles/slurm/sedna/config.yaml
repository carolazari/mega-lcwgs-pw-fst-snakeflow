cluster:
  mkdir -p results/slurm_logs/{rule} &&
  sbatch
    --exclude=node[32-36]
    --cpus-per-task={threads}
    --mem={resources.mem_mb}
    --time={resources.time}
    --job-name=smk-{rule}-{wildcards}
    --output=results/slurm_logs/{rule}/%j-{rule}-{wildcards}.out
    --error=results/slurm_logs/{rule}/%j-{rule}-{wildcards}.err
    --parsable
default-resources:
  - time="23:00:00"
  - mem_mb=4700
restart-times: 0
max-jobs-per-second: 10
max-status-checks-per-second: 50
local-cores: 1
latency-wait: 180
cores: 600
jobs: 1200
keep-going: True
rerun-incomplete: True
printshellcmds: True
use-conda: True
rerun-triggers: mtime
cluster-status: status-sacct.sh
cluster-cancel: scancel
cluster-cancel-nargs: 1000
set-threads:
  - calc_saf=4
  - calc_2dsfs=5
  - calc_2dsfs_winsfs=10
set-resources:
  - calc_saf:mem_mb=18800
  - calc_2dsfs:mem_mb=24000
  - calc_2dsfs_winsfs:mem_mb=7000
  - calc_1dsfs_realSFS: mem_mb=47000
